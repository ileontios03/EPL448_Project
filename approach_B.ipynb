{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5610b2ed-1dc1-4991-8da6-19c37e67124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, RobustScaler, PowerTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17743550-58d8-4c91-9471-9fb1e3e88bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('alzheimers_prediction_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b391a458-dba4-414a-9cd5-94b263ae06a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 59426 samples\n",
      "Test set size: 14857 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop the target column and store it separately\n",
    "X = df.drop(columns=['Alzheimer’s Diagnosis'])\n",
    "y = df['Alzheimer’s Diagnosis'].map({'No': 0, 'Yes': 1})  # convert to binary\n",
    "\n",
    "# Early train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e755057-69d4-4ecf-9d76-b3dcf9e96776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_Train (59426, 24)\n",
      "Shape of X_Test (14857, 24)\n",
      "Shape of Y_Train (59426,)\n",
      "Shape of Y_Test (14857,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_Train {X_train.shape}')\n",
    "print(f'Shape of X_Test {X_test.shape}')\n",
    "print(f'Shape of Y_Train {y_train.shape}')\n",
    "print(f'Shape of Y_Test {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b0861-eb90-4e4a-bfd0-260cfb70f1dd",
   "metadata": {},
   "source": [
    "### Define Feature Groups\r\n",
    "\r\n",
    "We start by grouping our dataset's features into categories based on their type and required preprocessing:\r\n",
    "\r\n",
    "- `num_features`: All numerical features.\r\n",
    "- `features_to_scale`: A subset of the numerical features that will either be scaled or unskewed, depending on the preprocessing pipeline (e.g., Age, Cognitive Test Score).\r\n",
    "- `cat_features`: All categorical features, identified by selecting columns with `object` dtype.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ab45aa-514e-4ee4-983e-acfe1e378566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "num_features = ['Age', 'Education Level', 'BMI', 'Cognitive Test Score']\n",
    "features_to_scale = ['Age', 'Cognitive Test Score']  # from your V1/V2\n",
    "cat_features = X.select_dtypes(include='object').columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f6b8a-98ac-462d-8a81-31ad7ef078a6",
   "metadata": {},
   "source": [
    "### Preprocessing Pipelines\n",
    "\n",
    "We create separate preprocessing pipelines for handling numerical and categorical features:\n",
    "\n",
    "- **Numerical Pipelines**:\n",
    "  - `num_pipeline_v1`: Scales numerical features using **RobustScaler**.\n",
    "  - `num_pipeline_v2`: Applies the **PowerTransformer (Yeo-Johnson)** to unskew numerical features.\n",
    "  \n",
    "- **Categorical Encoding**:\n",
    "  - `cat_pipeline`: Encodes categorical features using **OrdinalEncoder**, ensuring unknown values are handled gracefully.\n",
    "\n",
    "- **Column Transformers**:\n",
    "  - `preprocessor1`: Combines `num_pipeline_v1` for scaled features, leaves other numerical features as is, and applies `cat_pipeline` to categorical features.\n",
    "  - `preprocessor2`: Similar to `preprocessor1`, but uses `num_pipeline_v2` to unskew features.\n",
    "\n",
    "These transformations are applied within separate preprocessing pipelines to prepare the data before classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b55a96a-ed4a-4393-bf18-b6bcf3ab451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer, OrdinalEncoder\n",
    "\n",
    "# V1: RobustScaler\n",
    "num_pipeline_v1 = Pipeline([\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# V2: PowerTransformer (Yeo-Johnson)\n",
    "num_pipeline_v2 = Pipeline([\n",
    "    ('unskewer', PowerTransformer(method='yeo-johnson'))\n",
    "])\n",
    "\n",
    "# Categorical encoding (Ordinal)\n",
    "cat_pipeline = Pipeline([\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "# Preprocessor for V1\n",
    "preprocessor1 = ColumnTransformer([\n",
    "    ('num_scaled', num_pipeline_v1, features_to_scale),\n",
    "    ('num_passthrough', 'passthrough', list(set(num_features) - set(features_to_scale))),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "])\n",
    "\n",
    "# Preprocessor for V2\n",
    "preprocessor2 = ColumnTransformer([\n",
    "    ('num_unskewed', num_pipeline_v2, features_to_scale),\n",
    "    ('num_passthrough', 'passthrough', list(set(num_features) - set(features_to_scale))),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11254ae3-1d8a-4bfb-b212-80a4b817f8c1",
   "metadata": {},
   "source": [
    "### Classification Pipelines\r\n",
    "\r\n",
    "We define pipelines for AdaBoost and CatBoost classifiers, each with different preprocessing steps:\r\n",
    "\r\n",
    "- **AdaBoost Classifiers**:\r\n",
    "  - `pipeline_ada_v1`: Applies `preprocessor1` (which uses **RobustScaler** for scaling) and then trains an **AdaBoostClassifier**.\r\n",
    "  - `pipeline_ada_v2`: Uses `preprocessor2` (which applies **PowerTransformer** for unskewing) followed by an **AdaBoostClassifier**.\r\n",
    "\r\n",
    "- **CatBoost Classifiers**:\r\n",
    "  - `pipeline_cat_v1`: Similar to `pipeline_ada_v1`, but uses **CatBoostClassifier** for classification, with `silent=True` to suppress output.\r\n",
    "  - `pipeline_cat_v2`: Similar to `pipeline_ada_v2`, but uses **CatBoostClassifier** instead of AdaBoost.\r\n",
    "\r\n",
    "These pipelines streamline the application of preprocessing and classifier steps together.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b9fe616-7f92-44f5-8c44-dfc62eb2bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "pipeline_ada_v1 = Pipeline([\n",
    "    ('preprocessor', preprocessor1),\n",
    "    ('classifier', AdaBoostClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_cat_v1 = Pipeline([\n",
    "    ('preprocessor', preprocessor1),\n",
    "    ('classifier', CatBoostClassifier(silent=True, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_ada_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor2),\n",
    "    ('classifier', AdaBoostClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_cat_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor2),\n",
    "    ('classifier', CatBoostClassifier(silent=True, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e45a6-5386-4570-9871-b594f26dc588",
   "metadata": {},
   "source": [
    "### Hyperparameter Grids for GridSearchCV\n",
    "\n",
    "We define parameter grids for **GridSearchCV** to tune hyperparameters for the AdaBoost and CatBoost classifiers:\n",
    "\n",
    "- **AdaBoost Hyperparameter Grid** (`param_grid_ada`):\n",
    "  - `n_estimators`: Number of estimators to use (50, 100, 200).\n",
    "  - `learning_rate`: Learning rate for the classifier (0.01, 0.1, 1.0).\n",
    "  - `algorithm`: AdaBoost algorithm variant (only 'SAMME' is used here).\n",
    "\n",
    "- **CatBoost Hyperparameter Grid** (`param_grid_cat`):\n",
    "  - `depth`: Depth of the decision trees used (4, 6).\n",
    "  - `iterations`: Number of boosting iterations (100, 200).\n",
    "  - `learning_rate`: Learning rate for CatBoost (0.01, 0.1).\n",
    "\n",
    "These grids will be used in **GridSearchCV** to explore different hyperparameter combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b11f15-e45c-4d6f-b151-ead668b003f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Grid\n",
    "param_grid_ada = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 1.0],\n",
    "    'classifier__algorithm': ['SAMME']\n",
    "}\n",
    "\n",
    "# CatBoost Grid\n",
    "param_grid_cat = {\n",
    "    'classifier__depth': [4, 6],\n",
    "    'classifier__iterations': [100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914690b-6ab8-4d2d-8b3a-5b9e4e13bd02",
   "metadata": {},
   "source": [
    "### Running GridSearchCV for Hyperparameter Tuning\r\n",
    "\r\n",
    "In this step, we perform **GridSearchCV** for each pipeline, testing various hyperparameters for the **AdaBoost** and **CatBoost** classifiers. The process includes:\r\n",
    "\r\n",
    "- **5-fold cross-validation** for model evaluation.\r\n",
    "- Use of **weighted F1 score** to account for class imbalance.\r\n",
    "- For each model, we output:\r\n",
    "  - The **best hyperparameters** found during the search.\r\n",
    "  - The **best cross-validation score**.\r\n",
    "  - The **test set performance**.\r\n",
    "\r\n",
    "This step ensures that the best models with optimized hyperparameters are selected for further evaluation.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3525f765-9098-49a5-893d-77be861abb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for ada_v1...\n",
      "Best parameters for ada_v1: {'classifier__algorithm': 'SAMME', 'classifier__learning_rate': 1.0, 'classifier__n_estimators': 100}\n",
      "Best CV F1 score: 0.7291\n",
      "Test F1 score: 0.7337\n",
      "\n",
      "Running GridSearchCV for cat_v1...\n",
      "Best parameters for cat_v1: {'classifier__depth': 6, 'classifier__iterations': 100, 'classifier__learning_rate': 0.01}\n",
      "Best CV F1 score: 0.7275\n",
      "Test F1 score: 0.7305\n",
      "\n",
      "Running GridSearchCV for ada_v2...\n",
      "Best parameters for ada_v2: {'classifier__algorithm': 'SAMME', 'classifier__learning_rate': 1.0, 'classifier__n_estimators': 100}\n",
      "Best CV F1 score: 0.7291\n",
      "Test F1 score: 0.7337\n",
      "\n",
      "Running GridSearchCV for cat_v2...\n",
      "Best parameters for cat_v2: {'classifier__depth': 6, 'classifier__iterations': 100, 'classifier__learning_rate': 0.01}\n",
      "Best CV F1 score: 0.7275\n",
      "Test F1 score: 0.7305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register pipelines\n",
    "pipelines = {\n",
    "    'ada_v1': (pipeline_ada_v1, param_grid_ada),\n",
    "    'cat_v1': (pipeline_cat_v1, param_grid_cat),\n",
    "    'ada_v2': (pipeline_ada_v2, param_grid_ada),\n",
    "    'cat_v2': (pipeline_cat_v2, param_grid_cat)\n",
    "}\n",
    "\n",
    "# F1 scorer\n",
    "f1_weighted = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Run GridSearchCV\n",
    "best_estimators = {}\n",
    "\n",
    "for name, (pipeline, grid) in pipelines.items():\n",
    "    print(f\"Running GridSearchCV for {name}...\")\n",
    "    search = GridSearchCV(pipeline, grid, cv=5, scoring=f1_weighted, n_jobs=-1)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    best_estimators[name] = search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {search.best_params_}\")\n",
    "    print(f\"Best CV F1 score: {search.best_score_:.4f}\")\n",
    "\n",
    "    y_pred = search.predict(X_test)\n",
    "    print(f\"Test F1 score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b5836-98eb-4a0b-a22a-4302ec5ac8d0",
   "metadata": {},
   "source": [
    "### Results Discussion:\n",
    "\n",
    "The **GridSearchCV** results for the four pipelines (AdaBoost and CatBoost with both preprocessing approaches) provide valuable insights into the model performance:\n",
    "\n",
    "1. **AdaBoost - V1 and V2**:\n",
    "   - **Best Parameters**:\n",
    "     - `algorithm`: SAMME (this is the default and often works well for AdaBoost).\n",
    "     - `learning_rate`: 1.0 (indicating that a moderate learning rate works best for this dataset).\n",
    "     - `n_estimators`: 100 (number of weak learners).\n",
    "   - **Cross-validation (CV) F1 score**: **0.7291**\n",
    "     - This is the performance achieved using cross-validation, meaning the model performs relatively well in a 5-fold validation setup.\n",
    "   - **Test F1 score**: **0.7337**\n",
    "     - The model generalizes well to the unseen test set, with a slight improvement over the CV score, indicating minimal overfitting.\n",
    "\n",
    "2. **CatBoost - V1 and V2**:\n",
    "   - **Best Parameters**:\n",
    "     - `depth`: 6 (indicating the depth of the trees is optimized at 6).\n",
    "     - `iterations`: 100 (a standard number of iterations to ensure good performance without excessive training time).\n",
    "     - `learning_rate`: 0.01 (a low learning rate, suggesting the model is learning slowly but steadily, which is typical for CatBoost).\n",
    "   - **CV F1 score**: **0.7275**\n",
    "     - The model is slightly behind AdaBoost in terms of CV performance, though the difference is minimal.\n",
    "   - **Test F1 score**: **0.7305**\n",
    "     - The CatBoost model performs very similarly to AdaBoost on the test set, again showing good generalization.\n",
    "\n",
    "### Key Observations:\n",
    "- **AdaBoost** and **CatBoost** show **very similar performance** on the test set (both around 0.73), which suggests that both models are equally viable for this classification task.\n",
    "- The **AdaBoost** model is slightly more flexible, with a higher learning rate and using SAMME for boosting. It is performing well across both preprocessing strategies (V1 and V2).\n",
    "- **CatBoost** shows strong performance as well, especially with a low learning rate and limited depth, which is typical for the models behavior.\n",
    "\n",
    "### Next Steps:\n",
    "1. **Further Fine-Tuning**: There could be room for improvement in the models by exploring a wider range of hyperparameters or more advanced techniques like ensemble methods.\n",
    "2. **Model Selection**: Since the performance is so close, further evaluation (e.g., feature importance analysis or cross-validation with more folds) could help make the final decision on which model to proceed with.\n",
    "\n",
    "Overall, it seems both models are strong contenders, and further analysis will help in making the best choice for the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbdfffb-38d3-45dd-8254-a4d4bb6af3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
