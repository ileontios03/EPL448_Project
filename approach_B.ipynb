{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5610b2ed-1dc1-4991-8da6-19c37e67124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17743550-58d8-4c91-9471-9fb1e3e88bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('alzheimers_prediction_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b391a458-dba4-414a-9cd5-94b263ae06a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 59426 samples\n",
      "Test set size: 14857 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop the target column and store it separately\n",
    "X = df.drop(columns=['Alzheimer’s Diagnosis'])\n",
    "y = df['Alzheimer’s Diagnosis'].map({'No': 0, 'Yes': 1})  # convert to binary\n",
    "\n",
    "# Early train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e755057-69d4-4ecf-9d76-b3dcf9e96776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_Train (59426, 24)\n",
      "Shape of X_Test (14857, 24)\n",
      "Shape of Y_Train (59426,)\n",
      "Shape of Y_Test (14857,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_Train {X_train.shape}')\n",
    "print(f'Shape of X_Test {X_test.shape}')\n",
    "print(f'Shape of Y_Train {y_train.shape}')\n",
    "print(f'Shape of Y_Test {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ab45aa-514e-4ee4-983e-acfe1e378566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features in your dataset\n",
    "num_features = ['Age', 'BMI', 'Education Level', 'Cognitive Test Score']\n",
    "\n",
    "# Features to be scaled or unskewed (typically those with wider ranges or skewed distributions)\n",
    "features_to_scale = ['BMI', 'Cognitive Test Score']  # Adjust based on your EDA if needed\n",
    "\n",
    "# Categorical features in your dataset\n",
    "cat_features = [\n",
    "    'Country', 'Gender', 'Physical Activity Level', 'Smoking Status',\n",
    "    'Alcohol Consumption', 'Diabetes', 'Hypertension', 'Cholesterol Level',\n",
    "    'Family History of Alzheimer’s', 'Depression Level', 'Sleep Quality',\n",
    "    'Dietary Habits', 'Air Pollution Exposure', 'Employment Status',\n",
    "    'Marital Status', 'Genetic Risk Factor (APOE-ε4 allele)',\n",
    "    'Social Engagement Level', 'Income Level', 'Stress Levels',\n",
    "    'Urban vs Rural Living'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b55a96a-ed4a-4393-bf18-b6bcf3ab451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer, OrdinalEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# Preprocessing pipeline for numerical features that need only imputing (not scaling, nor unskewing) \n",
    "num_pipeline1 = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical features that need imputing + scaling\n",
    "num_pipeline2 = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical features that need imputing + unskewing\n",
    "num_pipeline3 = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('unskewer', PowerTransformer(method='yeo-johnson'))\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for categorical features\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b9fe616-7f92-44f5-8c44-dfc62eb2bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline for featureset v1 (impute + scale)\n",
    "preprocessor1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num1', num_pipeline1, list(set(num_features) - set(features_to_scale))),\n",
    "        ('num2', num_pipeline2, features_to_scale),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Preprocessing pipeline for featureset v2 (impute + unskew)\n",
    "preprocessor2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num1', num_pipeline1, list(set(num_features) - set(features_to_scale))),\n",
    "        ('num2', num_pipeline3, features_to_scale),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329798f9-53ec-442e-ad1a-f58101e7370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline([\n",
    "    ('preprocessor', preprocessor1),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "pipeline2 = Pipeline([\n",
    "    ('preprocessor', preprocessor1),\n",
    "    ('classifier', AdaBoostClassifier())\n",
    "])\n",
    "pipeline3 = Pipeline([\n",
    "    ('preprocessor', preprocessor2),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "pipeline4 = Pipeline([\n",
    "    ('preprocessor', preprocessor2),\n",
    "    ('classifier', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "pipelines = {\n",
    "    'svc_v1_pipeline': pipeline1,\n",
    "    'ada_v1_pipeline': pipeline2,\n",
    "    'svc_v2_pipeline': pipeline3,\n",
    "    'ada_v2_pipeline': pipeline4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c02c94c1-5b60-4c2a-aa03-6cfd06e59c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline([\n",
    "    ('preprocessor', preprocessor1),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "pipeline2 = Pipeline([\n",
    "    ('preprocessor', preprocessor1),\n",
    "    ('classifier', AdaBoostClassifier())\n",
    "])\n",
    "pipeline3 = Pipeline([\n",
    "    ('preprocessor', preprocessor2),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "pipeline4 = Pipeline([\n",
    "    ('preprocessor', preprocessor2),\n",
    "    ('classifier', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "pipelines = {\n",
    "    'svc_v1_pipeline': pipeline1,\n",
    "    'ada_v1_pipeline': pipeline2,\n",
    "    'svc_v2_pipeline': pipeline3,\n",
    "    'ada_v2_pipeline': pipeline4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40b11f15-e45c-4d6f-b151-ead668b003f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = [\n",
    "    {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'classifier__gamma': ['scale', 'auto']\n",
    "    }\n",
    "]\n",
    "param_grid2 = [\n",
    "    {\n",
    "        'classifier__n_estimators': [50, 100, 200, 500],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 1.0, 10],\n",
    "        'classifier__algorithm': ['SAMME']\n",
    "    }\n",
    "]\n",
    "\n",
    "param_grids = {\n",
    "    'svc_v1_pipeline': param_grid1,\n",
    "    'ada_v1_pipeline': param_grid2,\n",
    "    'svc_v2_pipeline': param_grid1,\n",
    "    'ada_v2_pipeline': param_grid2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0bf30-379f-47b1-9201-b8b9bb4fe9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for svc_v1_pipeline...\n"
     ]
    }
   ],
   "source": [
    "f1_score_weighted = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "best_estimators = {}\n",
    "for pipeline_name, pipeline in pipelines.items():\n",
    "    print(f\"Running GridSearchCV for {pipeline_name}...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grids[pipeline_name],\n",
    "        cv=5,\n",
    "        scoring=f1_score_weighted,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store best estimator\n",
    "    best_estimators[pipeline_name] = grid_search.best_estimator_\n",
    "    print(f\"Best Parameters for {pipeline_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validated Score: {grid_search.best_score_}\")\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    print(f\"Test F1 Score: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
