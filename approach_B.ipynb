{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5610b2ed-1dc1-4991-8da6-19c37e67124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, RobustScaler, PowerTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17743550-58d8-4c91-9471-9fb1e3e88bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('alzheimers_prediction_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b391a458-dba4-414a-9cd5-94b263ae06a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 59426 samples\n",
      "Test set size: 14857 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop the target column and store it separately\n",
    "X = df.drop(columns=['Alzheimer’s Diagnosis'])\n",
    "y = df['Alzheimer’s Diagnosis'].map({'No': 0, 'Yes': 1})  # convert to binary\n",
    "\n",
    "# Early train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e755057-69d4-4ecf-9d76-b3dcf9e96776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_Train (59426, 24)\n",
      "Shape of X_Test (14857, 24)\n",
      "Shape of Y_Train (59426,)\n",
      "Shape of Y_Test (14857,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_Train {X_train.shape}')\n",
    "print(f'Shape of X_Test {X_test.shape}')\n",
    "print(f'Shape of Y_Train {y_train.shape}')\n",
    "print(f'Shape of Y_Test {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ab45aa-514e-4ee4-983e-acfe1e378566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numerical and categorical features based on your dataset\n",
    "num_features = ['Age', 'Education Level', 'BMI', 'Cognitive Test Score']\n",
    "\n",
    "# Be careful with apostrophes and exact spellings\n",
    "cat_features = [col for col in X_train.columns if col not in num_features]\n",
    "\n",
    "# Features that may need scaling based on EDA (update these based on your actual EDA findings)\n",
    "features_to_scale = ['Age', 'BMI']  \n",
    "\n",
    "# The rest of numerical features\n",
    "num_features_no_scale = list(set(num_features) - set(features_to_scale))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b55a96a-ed4a-4393-bf18-b6bcf3ab451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline for numerical features that need only imputing (not scaling)\n",
    "num_pipeline1 = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical features that need imputing + scaling\n",
    "num_pipeline2 = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical features that need imputing + unskewing\n",
    "num_pipeline3 = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('unskewer', PowerTransformer(method='yeo-johnson'))\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for categorical features\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b9fe616-7f92-44f5-8c44-dfc62eb2bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline for featureset v1 (impute + scale)\n",
    "preprocessor1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num1', num_pipeline1, list(set(num_features) - set(features_to_scale))),\n",
    "        ('num2', num_pipeline2, features_to_scale),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Preprocessing pipeline for featureset v2 (impute + unskew)\n",
    "preprocessor2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num1', num_pipeline1, list(set(num_features) - set(features_to_scale))),\n",
    "        ('num2', num_pipeline3, features_to_scale),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329798f9-53ec-442e-ad1a-f58101e7370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines with different preprocessing steps and classifiers\n",
    "pipeline1 = Pipeline([\n",
    "    ('preprocessor', preprocessor1),\n",
    "    ('classifier', SVC(probability=True))\n",
    "])\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('preprocessor', preprocessor1),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "pipeline3 = Pipeline([\n",
    "    ('preprocessor', preprocessor2),\n",
    "    ('classifier', SVC(probability=True))\n",
    "])\n",
    "\n",
    "pipeline4 = Pipeline([\n",
    "    ('preprocessor', preprocessor2),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define different pipelines with different classifiers and preprocessing steps\n",
    "pipelines = {\n",
    "    'svc_v1_pipeline': pipeline1,\n",
    "    'rf_v1_pipeline': pipeline2,\n",
    "    'svc_v2_pipeline': pipeline3,\n",
    "    'rf_v2_pipeline': pipeline4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40b11f15-e45c-4d6f-b151-ead668b003f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameter grid for GridSearchCV for each model\n",
    "param_grid_svc = [\n",
    "    {'classifier__C': [0.1, 1, 10, 100], \n",
    "     'classifier__kernel': ['linear', 'rbf'], \n",
    "     'classifier__gamma': ['scale', 'auto']}\n",
    "]\n",
    "\n",
    "param_grid_rf = [\n",
    "    {'classifier__n_estimators': [100, 200, 300],\n",
    "     'classifier__max_depth': [None, 10, 20],\n",
    "     'classifier__min_samples_split': [2, 5, 10]}\n",
    "]\n",
    "\n",
    "param_grids = {\n",
    "    'svc_v1_pipeline': param_grid_svc,\n",
    "    'rf_v1_pipeline': param_grid_rf,\n",
    "    'svc_v2_pipeline': param_grid_svc,\n",
    "    'rf_v2_pipeline': param_grid_rf\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0bf30-379f-47b1-9201-b8b9bb4fe9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for svc_v1_pipeline...\n"
     ]
    }
   ],
   "source": [
    "def f1_score_weighted(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Loop through each pipeline and perform GridSearchCV\n",
    "best_estimators = {}\n",
    "for pipeline_name, pipeline in pipelines.items():\n",
    "    print(f\"Running GridSearchCV for {pipeline_name}...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline, \n",
    "        param_grid=param_grids[pipeline_name], \n",
    "        cv=5, \n",
    "        scoring='f1_weighted', \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the best estimator and results for each pipeline\n",
    "    best_estimators[pipeline_name] = grid_search.best_estimator_\n",
    "    print(f\"Best Parameters for {pipeline_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validated Score for {pipeline_name}: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Make predictions using the best estimator\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "    print(f\"F1 Score on test set: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
